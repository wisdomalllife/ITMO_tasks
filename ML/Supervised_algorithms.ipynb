{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LmXy9r7SbzJR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "me6SyaUKOu0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import arff\n",
        "from sklearn import preprocessing as prepro\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "mDFj69CQO3Hi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "YQDtC2h3Mr4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://openml.org/search?type=data&sort=runs&status=active&id=37"
      ],
      "metadata": {
        "id": "VhHh5ZYKRpS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://api.openml.org/data/v1/download/37/diabetes.arff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgri8IPIB9hP",
        "outputId": "7098db55-f24f-4c4e-907a-eeb6a10b831e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-04 13:06:06--  https://api.openml.org/data/v1/download/37/diabetes.arff\n",
            "Resolving api.openml.org (api.openml.org)... 131.155.11.11\n",
            "Connecting to api.openml.org (api.openml.org)|131.155.11.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37419 (37K) [text/plain]\n",
            "Saving to: ‘diabetes.arff’\n",
            "\n",
            "diabetes.arff       100%[===================>]  36.54K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-11-04 13:06:07 (360 KB/s) - ‘diabetes.arff’ saved [37419/37419]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJNM3Nf8Ma0W",
        "outputId": "0d8f0983-6577-4a21-ce71-eacde6077c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   preg   plas  pres  skin   insu  mass   pedi   age               class\n",
            "0   6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0  b'tested_positive'\n",
            "1   1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0  b'tested_negative'\n",
            "2   8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0  b'tested_positive'\n",
            "3   1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0  b'tested_negative'\n",
            "4   0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0  b'tested_positive'\n"
          ]
        }
      ],
      "source": [
        "data = arff.loadarff('diabetes.arff')\n",
        "df = pd.DataFrame(data[0])\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['class'].replace([b'tested_negative',b'tested_positive'], [0,1], inplace=True)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0LsRinFk5uO",
        "outputId": "84705f7c-a931-438c-9ca9-f1bec91956ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   preg   plas  pres  skin   insu  mass   pedi   age  class\n",
            "0   6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0      1\n",
            "1   1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0      0\n",
            "2   8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0      1\n",
            "3   1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0      0\n",
            "4   0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "3hpNsW5phJ9k",
        "outputId": "6df823c3-6e60-4044-b3b4-7a6a8094ea50"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "preg     0\n",
              "plas     0\n",
              "pres     0\n",
              "skin     0\n",
              "insu     0\n",
              "mass     0\n",
              "pedi     0\n",
              "age      0\n",
              "class    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>preg</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>plas</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pres</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skin</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>insu</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mass</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pedi</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = prepro.MinMaxScaler().fit(df)\n",
        "normalized_attr = scaler.transform(df)\n",
        "normalized_df = pd.DataFrame(normalized_attr, columns=df.columns)\n",
        "print(normalized_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy735zE_mA4Z",
        "outputId": "88908bd0-4730-43dd-8981-34b2a48727c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       preg      plas      pres      skin      insu      mass      pedi  \\\n",
            "0  0.352941  0.743719  0.590164  0.353535  0.000000  0.500745  0.234415   \n",
            "1  0.058824  0.427136  0.540984  0.292929  0.000000  0.396423  0.116567   \n",
            "2  0.470588  0.919598  0.524590  0.000000  0.000000  0.347243  0.253629   \n",
            "3  0.058824  0.447236  0.540984  0.232323  0.111111  0.418778  0.038002   \n",
            "4  0.000000  0.688442  0.327869  0.353535  0.198582  0.642325  0.943638   \n",
            "\n",
            "        age  class  \n",
            "0  0.483333    1.0  \n",
            "1  0.166667    0.0  \n",
            "2  0.183333    1.0  \n",
            "3  0.000000    0.0  \n",
            "4  0.200000    1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalized_df.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "bAME8qm7s_ZE"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalized_df.hist()\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "FO2Kif5mjLCd"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "CmpP0goGO6xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kernel functions\n",
        "def uniform(d, h):\n",
        "    return np.where(np.abs(d / h)<= 0.5, 1, 0)\n",
        "\n",
        "def triangular(d, h):\n",
        "    return np.where(np.abs(d) <= h, 1 - np.abs(d / h), 0)\n",
        "\n",
        "def epanechnikov(d, h):\n",
        "    return np.where(np.abs(d) <= h, 3 / 4 * (1 - (d / h) ** 2), 0)\n",
        "\n",
        "def gaussian(d, h):\n",
        "    return 1/np.sqrt(2 * np.pi)*np.exp(-0.5*(d / h)**2)"
      ],
      "metadata": {
        "id": "mH3_JGdjpRXq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN:\n",
        "    def __init__(self, k=3, kernel=uniform, window='fixed'):\n",
        "        self.k = k\n",
        "        self.kernel = kernel\n",
        "        self.window = window\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = np.asarray(X)\n",
        "        self.y_train = np.asarray(y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.asarray(X)\n",
        "        predictions = []\n",
        "\n",
        "        for x in X:\n",
        "            # Calculate distances\n",
        "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
        "\n",
        "            # window size\n",
        "            if self.window == 'fixed': # the window size is constant for all points.\n",
        "                size = np.mean(distances)\n",
        "            else: # the window size can change depending on the local data distribution.\n",
        "                size = np.partition(distances, self.k)[self.k]  # k-th smallest distance\n",
        "\n",
        "            # Apply kernel weights\n",
        "            weights = self.kernel(distances, size)\n",
        "\n",
        "            # Get the indices of the k nearest neighbors\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_weights = weights[k_indices]\n",
        "            # Extract the labels of the k nearest neighbors\n",
        "            k_labels = self.y_train[k_indices]\n",
        "            k_labels = np.array(k_labels).flatten()\n",
        "            # Weighted voting (most common class label)\n",
        "            label_sum = np.bincount(k_labels, weights=k_weights, minlength=2)\n",
        "            predicted_label = np.argmax(label_sum)\n",
        "            predictions.append(predicted_label)\n",
        "\n",
        "        return np.array(predictions)"
      ],
      "metadata": {
        "id": "gYkZlfxcCY_V"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "xPfMBYweIaHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(normalized_df,test_size=0.3,random_state=0,stratify=normalized_df['class'])\n",
        "\n",
        "train_X = train[train.columns[:8]]\n",
        "train_Y = train[train.columns[8:]]\n",
        "\n",
        "test_X = test[test.columns[:8]]\n",
        "test_Y = test[test.columns[8:]]\n",
        "\n",
        "X = normalized_df[normalized_df.columns[:8]]\n",
        "Y = normalized_df['class']\n",
        "\n",
        "train_X = train_X.to_numpy().astype(float)\n",
        "train_Y = train_Y.to_numpy().astype(int).flatten()\n",
        "test_X = test_X.to_numpy().astype(float)\n",
        "test_Y = test_Y.to_numpy().astype(int).flatten()\n",
        "X = X.to_numpy().astype(float)\n",
        "Y = Y.to_numpy().astype(int)\n",
        "\n",
        "knn = KNN()\n",
        "knn.fit(train_X, train_Y)\n",
        "\n",
        "predictions = knn.predict(test_X)\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2d7tJ5yHYPD",
        "outputId": "aeddfae8-6f63-4ac1-b1f2-2ba533e93031"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 1 0\n",
            " 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1\n",
            " 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 0\n",
            " 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1\n",
            " 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning"
      ],
      "metadata": {
        "id": "LmXErTr466F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation function\n",
        "def cross_validate(model, X, y, folds=6):\n",
        "    kfold = KFold(n_splits=folds, shuffle=True, random_state=22) # k=folds, split the data into k equal parts\n",
        "    f1_scores = []\n",
        "    for train_index, valid_index in kfold.split(X):\n",
        "      # Splitting Data\n",
        "        X_train, X_test = X[train_index], X[valid_index]\n",
        "        y_train, y_test = y[train_index], y[valid_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        f1_scores.append(f1_score(y_test, model.predict(X_test)))\n",
        "\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "# hyperparameter_tuning\n",
        "def hyperparameter_tuning_knn(X, y):\n",
        "    best_score, best_params, scores = 0, None, []\n",
        "    kernels = [uniform, triangular, epanechnikov, gaussian]\n",
        "    windows = ['fixed', 'neighbor']\n",
        "    k_values = [i for i in range(1,50,2)]\n",
        "\n",
        "    for kernel in kernels:\n",
        "        for window in windows:\n",
        "            for k in k_values:\n",
        "                model = KNN(k=k, kernel=kernel, window=window)\n",
        "                score = cross_validate(model, X, y)\n",
        "                scores.append({'k': k, 'kernel': kernel.__name__, 'window': window, 'f1': float(score)})\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {'k': k, 'kernel': kernel.__name__, 'window': window}\n",
        "\n",
        "    return best_params, best_score, scores"
      ],
      "metadata": {
        "id": "QdRmTMjqPcP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params, best_score, scores = hyperparameter_tuning_knn(X, Y)\n",
        "\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "print(\"Best F1 score:\", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl3V6fTEPk3Z",
        "outputId": "4b96e24f-bdcf-4f5e-9836-2b7397eaa35d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'k': 15, 'kernel': 'triangular', 'window': 'fixed'}\n",
            "Best F1 score: 0.5995206609860827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent"
      ],
      "metadata": {
        "id": "DVXBn2jIVkSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models. It works by iteratively adjusting the model parameters in the direction of the negative gradient of the cost function.\n",
        "\n",
        "• Batch Gradient Descent: The entire dataset is used to compute the gradients once per iteration.\n",
        "\n",
        "• Stochastic Gradient Descent: The model updates its parameters after calculating the gradient for each individual sample.\n",
        "\n",
        "• Mini-Batch Gradient Descent: The dataset is shuffled at the beginning of each iteration, and gradients are computed using small batches of data."
      ],
      "metadata": {
        "id": "rKTCYNxzEsip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, method='batch', batch_size=32):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.method = method\n",
        "        self.batch_size = batch_size\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Number of samples and features\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient Descent\n",
        "        for _ in range(self.n_iterations):\n",
        "            if self.method == 'batch':\n",
        "                # Calculate the linear model prediction for all data\n",
        "                y_predicted = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "                # Compute gradients\n",
        "                dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "                db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "                # Update weights and bias\n",
        "                self.weights -= self.learning_rate * dw\n",
        "                self.bias -= self.learning_rate * db\n",
        "\n",
        "            elif self.method == 'stochastic':\n",
        "                # Update for each sample\n",
        "                for i in range(n_samples):\n",
        "                    y_predicted = np.dot(X[i], self.weights) + self.bias\n",
        "\n",
        "                    # Compute gradients for the current sample\n",
        "                    dw = (y_predicted - y[i]) * X[i]\n",
        "                    db = y_predicted - y[i]\n",
        "\n",
        "                    # Update weights and bias\n",
        "                    self.weights -= self.learning_rate * dw\n",
        "                    self.bias -= self.learning_rate * db\n",
        "\n",
        "            elif self.method == 'mini-batch':\n",
        "              # Shuffle the data\n",
        "                indices = np.random.permutation(n_samples)\n",
        "                for i in range(0, n_samples, self.batch_size):\n",
        "                    batch_indices = indices[i:i + self.batch_size]\n",
        "                    X_batch = X[batch_indices]\n",
        "                    y_batch = y[batch_indices]\n",
        "                    predictions = X_batch.dot(self.weights)\n",
        "                    errors = predictions - y_batch.flatten()\n",
        "                    gradient = (X_batch.T.dot(errors)) / len(y_batch)\n",
        "                    self.weights -= self.learning_rate * gradient\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias"
      ],
      "metadata": {
        "id": "DaoswOdmY1OE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "ZIGJfvn5VTw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GradientDescent(learning_rate=0.01, n_iterations=100, method='mini-batch', batch_size=2)\n",
        "model.fit(train_X, train_Y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(test_X)\n",
        "predictions = (predictions > 0.5).astype(int)\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox24opLiVVbY",
        "outputId": "bd09660e-aa4a-4ccf-bea5-34d38c996273"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning"
      ],
      "metadata": {
        "id": "ieQgnyxb6P2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation function\n",
        "def cross_validate(model, X, y, folds=6):\n",
        "    kfold = KFold(n_splits=folds, shuffle=True, random_state=22) # k=folds, split the data into k equal parts\n",
        "    f1_scores = []\n",
        "    for train_index, valid_index in kfold.split(X):\n",
        "      # Splitting Data\n",
        "        X_train, X_test = X[train_index], X[valid_index]\n",
        "        y_train, y_test = y[train_index], y[valid_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_test)\n",
        "        predictions = (predictions > 0.5).astype(int)\n",
        "        f1_scores.append(f1_score(y_test, predictions))\n",
        "\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "# hyperparameter_tuning\n",
        "def hyperparameter_tuning_GD(X, y):\n",
        "    best_score, best_params, scores = 0, None, []\n",
        "    methods = ['mini-batch', 'stochastic', 'batch']\n",
        "    learning_rates = [0.01, 0.1]\n",
        "    batch_sizes = [8, 16, 32]\n",
        "\n",
        "    for method in methods:\n",
        "        for batch_size in batch_sizes:\n",
        "            for learning_rate in learning_rates:\n",
        "                model = GradientDescent(learning_rate=learning_rate, method=method, batch_size=batch_size)\n",
        "                score = cross_validate(model, X, y)\n",
        "                scores.append({'learning_rate': learning_rate, 'method': method, 'batch_size':batch_size, 'f1': float(score)})\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {'learning_rate': learning_rate, 'method': method, 'batch_size':batch_size}\n",
        "\n",
        "    return best_params, best_score, scores"
      ],
      "metadata": {
        "id": "8yBnrkue4aeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params, best_score, scores = hyperparameter_tuning_GD(X, Y)\n",
        "\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "print(\"Best F1 score:\", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8s3N3XN8ZoE",
        "outputId": "0f3770bc-1170-4288-a84a-0c74f00c9759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'learning_rate': 0.01, 'method': 'stochastic', 'batch_size': 8}\n",
            "Best F1 score: 0.6423380716726421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVD"
      ],
      "metadata": {
        "id": "IGe4UsBsVowP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SVD:\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.U = None  # Left singular vectors\n",
        "        self.S = None  # Singular values (as a diagonal matrix)\n",
        "        self.Vt = None  # Right singular vectors (transposed)\n",
        "        self.coef = None  # Coefficients for the linear classifier\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Center the data\n",
        "        X_centered = X - np.mean(X, axis=0)\n",
        "\n",
        "        # Compute SVD\n",
        "        self.U, self.S, self.Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
        "\n",
        "        # Select the top n_components\n",
        "        self.U = self.U[:, :self.n_components]\n",
        "        self.S = np.diag(self.S[:self.n_components])\n",
        "        self.Vt = self.Vt[:self.n_components, :]\n",
        "\n",
        "        # Fit a linear regression model to the transformed data\n",
        "        # We'll use the normal equation for linear regression\n",
        "        self.coef = np.linalg.pinv(self.U) @ y  # Use pseudo-inverse for linear regression\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_centered = X - np.mean(X, axis=0)\n",
        "        U_new = X_centered @ self.Vt.T[:, :self.n_components]\n",
        "\n",
        "        # Using the linear model to make predictions\n",
        "        return (U_new @ self.coef > 0).astype(int)"
      ],
      "metadata": {
        "id": "-xg8JM7dOgzV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "9GOg4KwQ9tKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SVD(n_components=20)\n",
        "model.fit(train_X, train_Y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(test_X).flatten()\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOR1E7_19m-f",
        "outputId": "b5c832dd-6e2b-41ad-d7b1-4ab74050c4e7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 1 1 0\n",
            " 1 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1\n",
            " 1 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0\n",
            " 0 0 0 1 1 0 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning"
      ],
      "metadata": {
        "id": "7_4X3ib89vUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation function\n",
        "def cross_validate(model, X, y, folds=6):\n",
        "    kfold = KFold(n_splits=folds, shuffle=True, random_state=22) # k=folds, split the data into k equal parts\n",
        "    f1_scores = []\n",
        "    for train_index, valid_index in kfold.split(X):\n",
        "      # Splitting Data\n",
        "        X_train, X_test = X[train_index], X[valid_index]\n",
        "        y_train, y_test = y[train_index], y[valid_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_test).flatten()\n",
        "        f1_scores.append(f1_score(y_test, predictions))\n",
        "\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "# hyperparameter_tuning\n",
        "def hyperparameter_tuning_SVD(X, y):\n",
        "    best_score, best_params, scores = 0, None, []\n",
        "    components = [i for i in range(1,20)]\n",
        "\n",
        "    for n_components in components:\n",
        "        model = SVD(n_components=n_components)\n",
        "        score = cross_validate(model, X, y)\n",
        "        scores.append({'n_components': n_components, 'f1': float(score)})\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = {'n_components': n_components}\n",
        "\n",
        "    return best_params, best_score, scores"
      ],
      "metadata": {
        "id": "8EKdMNQy9wxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params, best_score, scores = hyperparameter_tuning_SVD(X, Y)\n",
        "\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "print(\"Best F1 score:\", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NDq1zINA-Jl",
        "outputId": "d1c5b0c4-6f5c-4c98-b18f-e3332330c817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'n_components': 8}\n",
            "Best F1 score: 0.6758723253703632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost"
      ],
      "metadata": {
        "id": "b37P-q0KVbxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define a Weak Classifier: a decision stump as weak classifier.\n",
        "\n",
        "2. Implement the AdaBoost Algorithm: iteratively train weak classifiers and adjust their weights based on their performance."
      ],
      "metadata": {
        "id": "Izk1woUckgNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionStump:\n",
        "    def fit(self, X, y, weights):\n",
        "        n_samples, n_features = X.shape\n",
        "        best_stump = {}\n",
        "        min_error = float('inf')\n",
        "\n",
        "        for feature_index in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in thresholds:\n",
        "                for inequality in ['lt', 'gt']:\n",
        "                    predictions = self.predict(X, feature_index, threshold, inequality)\n",
        "                    errors = predictions != y\n",
        "                    weighted_error = np.dot(weights, errors)\n",
        "\n",
        "                    if weighted_error < min_error:\n",
        "                        min_error = weighted_error\n",
        "                        best_stump['feature_index'] = feature_index\n",
        "                        best_stump['threshold'] = threshold\n",
        "                        best_stump['inequality'] = inequality\n",
        "\n",
        "        return best_stump\n",
        "\n",
        "    def predict(self, X, feature_index, threshold, inequality):\n",
        "        predictions = np.ones(X.shape[0])\n",
        "        if inequality == 'lt':\n",
        "            predictions[X[:, feature_index] <= threshold] = -1\n",
        "        else:\n",
        "            predictions[X[:, feature_index] > threshold] = -1\n",
        "        return predictions\n",
        "\n",
        "class AdaBoost:\n",
        "    def __init__(self, n_estimators=50):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.alphas = []\n",
        "        self.stumps = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Convert y from {0, 1} to {-1, 1}\n",
        "        y = np.where(y == 0, -1, 1)\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "        weights = np.ones(n_samples) / n_samples  # Initialize weights\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            stump = DecisionStump()\n",
        "            best_stump = stump.fit(X, y, weights)\n",
        "            predictions = stump.predict(X, best_stump['feature_index'], best_stump['threshold'], best_stump['inequality'])\n",
        "\n",
        "            # Calculate the error and alpha (weight of the weak classifier)\n",
        "            errors = predictions != y\n",
        "            error_rate = np.dot(weights, errors)\n",
        "\n",
        "            # Avoid division by zero\n",
        "            if error_rate == 0:\n",
        "                error_rate = 1e-10\n",
        "\n",
        "            alpha = 0.5 * np.log((1 - error_rate) / error_rate)\n",
        "            self.alphas.append(alpha)\n",
        "            self.stumps.append(best_stump)\n",
        "\n",
        "            # Update weights\n",
        "            weights *= np.exp(-alpha * y * predictions)\n",
        "            weights /= np.sum(weights)  # Normalize to sum to 1\n",
        "\n",
        "    def predict(self, X):\n",
        "        final_predictions = np.zeros(X.shape[0])\n",
        "        for alpha, stump in zip(self.alphas, self.stumps):\n",
        "            # Create a new instance of DecisionStump for prediction\n",
        "            stump_instance = DecisionStump()\n",
        "            predictions = stump_instance.predict(X, stump['feature_index'], stump['threshold'], stump['inequality'])\n",
        "            final_predictions += alpha * predictions\n",
        "\n",
        "        return np.where(final_predictions > 0, 1, 0)  # Convert back to {0, 1}\n"
      ],
      "metadata": {
        "id": "yUxn5MhydT9d"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "-gJxllyOapdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Initialize and fit AdaBoost\n",
        "ada_boost = AdaBoost(n_estimators=1)\n",
        "ada_boost.fit(train_X, train_Y)\n",
        "\n",
        " # Predictions\n",
        "predictions = ada_boost.predict(test_X)\n",
        "print(\"Predictions:\", predictions.astype(int))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6D2c7bVW3Ry",
        "outputId": "e1dc91ce-dcbf-4850-fff9-4a1e2639dfc4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0\n",
            " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
            " 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
            " 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning"
      ],
      "metadata": {
        "id": "5FLm-75gZ6La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation function\n",
        "def cross_validate(model, X, y, folds=6):\n",
        "    kfold = KFold(n_splits=folds, shuffle=True, random_state=22) # k=folds, split the data into k equal parts\n",
        "    f1_scores = []\n",
        "    for train_index, valid_index in kfold.split(X):\n",
        "      # Splitting Data\n",
        "        X_train, X_test = X[train_index], X[valid_index]\n",
        "        y_train, y_test = y[train_index], y[valid_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        predictions = model.predict(X_test)\n",
        "        f1_scores.append(f1_score(y_test, predictions))\n",
        "\n",
        "    return np.mean(f1_scores)\n",
        "\n",
        "# hyperparameter_tuning\n",
        "def hyperparameter_tuning_Ada(X, y):\n",
        "    best_score, best_params, scores = 0, None, []\n",
        "    clf = [i for i in range(1,50)]\n",
        "\n",
        "    for n_estimators in clf:\n",
        "        model = AdaBoost(n_estimators=n_estimators)\n",
        "        score = cross_validate(model, X, y)\n",
        "        scores.append({'n_estimators':n_estimators, 'f1': float(score)})\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = {'n_estimators': n_estimators}\n",
        "\n",
        "    return best_params, best_score, scores"
      ],
      "metadata": {
        "id": "1snRBis4ZzP5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params, best_score, scores = hyperparameter_tuning_Ada(X, Y)\n",
        "\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "print(\"Best F1 score:\", best_score)"
      ],
      "metadata": {
        "id": "gZ77NmP2aA8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ab647f-fd6f-40df-8d4c-6f3e93efc21c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'n_estimators': 49}\n",
            "Best F1 score: 0.6903474039074696\n"
          ]
        }
      ]
    }
  ]
}